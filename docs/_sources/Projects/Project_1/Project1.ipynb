{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8772e785",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61010d6d",
   "metadata": {},
   "source": [
    "For this project, I'm using the [Bicycle Counts ](https://data.cityofnewyork.us/Transportation/Bicycle-Counts/uczf-rk3c/about_data) dataset from NYC Open Data, which counts the number of bicyclists that pass a given point (over East River bridges, for example). I took just the first 100,000 rows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675b2110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['countid', 'id', 'date', 'counts', 'status'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"bicycle_counts_100k.csv\", low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf38957",
   "metadata": {},
   "source": [
    "I imported pandas, our data management package. Then I read in the csv I downloaded and look at the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ec10a",
   "metadata": {},
   "source": [
    "## First, the easy way using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487be53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cyclists per observation: 30.02\n",
      "Median cyclists per observation: 20.00\n",
      "Mode cyclists per observation: 0\n"
     ]
    }
   ],
   "source": [
    "df[\"counts\"] = pd.to_numeric(df[\"counts\"], errors=\"coerce\")\n",
    "\n",
    "mean_counts = df[\"counts\"].mean()\n",
    "median_counts = df[\"counts\"].median()\n",
    "mode_counts = df[\"counts\"].mode()[0]\n",
    "\n",
    "print(f\"Mean cyclists per observation: {mean_counts:.2f}\")\n",
    "print(f\"Median cyclists per observation: {median_counts:.2f}\")\n",
    "print(f\"Mode cyclists per observation: {mode_counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263ed30",
   "metadata": {},
   "source": [
    "With pandas, it's really simple to compute the mean, median, and mode; we simply use the functions with those names within pandas. Thus, we have our mean, median, and mode of cyclist counts:\n",
    "mean = 30.02, median = 20.00, mode = 0 (most common is 0 observed cyclists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925c885",
   "metadata": {},
   "source": [
    "## Now, the hard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6033a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 30.02199\n",
      "Median: 20.0\n",
      "Mode: 0\n"
     ]
    }
   ],
   "source": [
    "counts_dict = {}\n",
    "\n",
    "with open(\"bicycle_counts_100k.csv\", \"r\") as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        val_str = line.split(\",\")[3].strip().strip('\"')\n",
    "        if val_str.isdigit():\n",
    "            val = int(val_str)\n",
    "            counts_dict[val] = counts_dict.get(val, 0) + 1\n",
    "\n",
    "counts = []\n",
    "for val, freq in counts_dict.items():\n",
    "    counts.extend([val] * freq)\n",
    "\n",
    "counts.sort()\n",
    "\n",
    "n = len(counts)\n",
    "mean_counts = sum(counts) / n\n",
    "\n",
    "if n % 2 == 1:\n",
    "    median_counts = counts[n // 2]\n",
    "else:\n",
    "    median_counts = (counts[n // 2 - 1] + counts[n // 2]) / 2\n",
    "\n",
    "mode_counts = max(counts_dict, key=counts_dict.get)\n",
    "\n",
    "print(\"Mean:\", mean_counts)\n",
    "print(\"Median:\", median_counts)\n",
    "print(\"Mode:\", mode_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1814a",
   "metadata": {},
   "source": [
    "I used simple Python functions, such as isdigit, split, strip and sort, as well as first a dictionary and then a list, to compute the mean, median and mode the hard way, without using pandas, statistics, or any other added functionality. The results are the same. Since we know the fourth column is 'counts' and it includes a number in each row, we don't need to do extra validation at the beginning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b7d31",
   "metadata": {},
   "source": [
    "## Finally, data visualization (using pandas but only standard library for drawing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ca718",
   "metadata": {},
   "source": [
    "First, I created a histogram of the distribution of bicycle counts within these first 100k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0bbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of Bicycle Counts\n",
      "\n",
      "    0â€“   30 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   30â€“   60 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   60â€“   90 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   90â€“  120 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  120â€“  150 | â–ˆâ–ˆ\n",
      "  150â€“  180 | \n",
      "  180â€“  210 | \n",
      "  210â€“  240 | \n",
      "  240â€“  270 | \n",
      "  270â€“  300 | \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"bicycle_counts_100k.csv\", low_memory=False)\n",
    "df[\"counts\"] = pd.to_numeric(df[\"counts\"], errors=\"coerce\")\n",
    "counts = df[\"counts\"].dropna().astype(int).tolist()\n",
    "\n",
    "min_val, max_val = min(counts), max(counts)\n",
    "bin_size = (max_val - min_val) // 10 or 1\n",
    "\n",
    "print(\"\\nDistribution of Bicycle Counts\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    low = min_val + i * bin_size\n",
    "    high = low + bin_size\n",
    "    count_in_bin = sum(low <= x < high for x in counts)\n",
    "    bar = \"â–ˆ\" * (count_in_bin // 500)\n",
    "    print(f\"{low:5d}â€“{high:5d} | {bar}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf1994",
   "metadata": {},
   "source": [
    "Then, I created a chart of the top 8 location ID's by average bicycle count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961b64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 100000 | non-null counts: 100000\n",
      "Columns: ['countid', 'id', 'date', 'counts', 'status']\n",
      "\n",
      "Average hourly bicycle counts â€” top IDs\n",
      "\n",
      "100009427 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 72 \n",
      "100009428 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 66 \n",
      "100062893 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 53 \n",
      "100047029 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 53 \n",
      "300028963 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 46 \n",
      "300020904 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 42 \n",
      "300020241 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 42 \n",
      "100010019 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 37 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"bicycle_counts_100k.csv\", low_memory=False)\n",
    "df[\"counts\"] = pd.to_numeric(df[\"counts\"], errors=\"coerce\")\n",
    "\n",
    "print(\"Rows:\", len(df), \"| non-null counts:\", df[\"counts\"].notna().sum())\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "by_id = pd.Series(dtype=float)\n",
    "if \"id\" in df.columns:\n",
    "    by_id = (\n",
    "        df.dropna(subset=[\"counts\"])\n",
    "        .groupby(\"id\", dropna=True)[\"counts\"]\n",
    "        .mean()\n",
    "        .dropna()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "top = by_id.head(8)\n",
    "\n",
    "\n",
    "def draw_bar_chart(pairs, title, width=40, label=\"\"):\n",
    "    if not pairs:\n",
    "        print(f\"\\n{title}\\n(no data to display)\")\n",
    "        return\n",
    "    max_label_len = max(len(k) for k, _ in pairs)\n",
    "    max_val = max(v for _, v in pairs) if pairs else 1.0\n",
    "    print(f\"\\n{title}\\n\")\n",
    "    for k, v in pairs:\n",
    "        n = int(round((v / max_val) * width)) if max_val > 0 else 0\n",
    "        bar = \"â–ˆ\" * n\n",
    "        print(f\"{k:<{max_label_len}} | {bar} {v:.0f} {label}\")\n",
    "\n",
    "\n",
    "if not top.empty:\n",
    "    data = [(str(i), float(v)) for i, v in top.items()]\n",
    "    draw_bar_chart(data, \"Average hourly bicycle counts â€” top IDs\")\n",
    "else:\n",
    "    counts = df[\"counts\"].dropna().astype(float)\n",
    "    if counts.empty:\n",
    "        print(\"\\nNo non-null numeric 'counts' found to visualize.\")\n",
    "    else:\n",
    "        bins = 8\n",
    "        cmin, cmax = counts.min(), counts.max()\n",
    "        if cmax == cmin:\n",
    "            edges = [cmin, cmax + 1]\n",
    "            hist = [len(counts)]\n",
    "        else:\n",
    "            edges = [cmin + i * (cmax - cmin) / bins for i in range(bins + 1)]\n",
    "            hist = [0] * bins\n",
    "            for x in counts:\n",
    "                idx = min(int((x - cmin) / (cmax - cmin) * bins), bins - 1)\n",
    "                hist[idx] += 1\n",
    "        pairs = []\n",
    "        for i in range(len(hist)):\n",
    "            lo = edges[i]\n",
    "            hi = edges[i + 1]\n",
    "            label = f\"[{int(lo)}â€“{int(hi)}]\"\n",
    "            pairs.append((label, hist[i]))\n",
    "        draw_bar_chart(\n",
    "            pairs, \"Distribution of hourly bicycle counts (text histogram)\", label=\"obs\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f7b1d",
   "metadata": {},
   "source": [
    "Finally, I created a sparkline of average counts by day. Since I only took the first 100k rows and it only includes data for June and July 2023, and thus doesn't differ too much, none of this analysis is the most useful, but it's still interesting to explore, and would be more interesting to do this with the full dataset of 7 million rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9130c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Daily Bicycle Counts (ðŸš² = 1 count)\n",
      "\n",
      "2023-06-02: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-03: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-04: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-05: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-06: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-07: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-08: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-09: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-10: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-11: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-12: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-13: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-14: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-15: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-16: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-17: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-18: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-19: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-20: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-21: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-22: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-23: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-24: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-25: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-26: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-27: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-28: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-29: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-06-30: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-01: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-02: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-03: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-04: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-05: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-06: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-07: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-08: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-09: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-10: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-11: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-12: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-13: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-14: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-15: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-16: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-17: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-18: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-19: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-20: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-21: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-22: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-23: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-24: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-25: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-26: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-27: \n",
      "2023-07-28: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n",
      "2023-07-29: ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²ðŸš²\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"bicycle_counts_100k.csv\", low_memory=False)\n",
    "df[\"counts\"] = pd.to_numeric(df[\"counts\"], errors=\"coerce\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "by_day = df.groupby(df[\"date\"].dt.date)[\"counts\"].mean().dropna()\n",
    "\n",
    "print(\"\\nAverage Daily Bicycle Counts (ðŸš² = 1 count)\\n\")\n",
    "for day, val in by_day.items():\n",
    "    bikes = int(val)\n",
    "    bar = \"ðŸš²\" * bikes\n",
    "    print(f\"{day}: {bar}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6fd12",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project explored the NYC DOT *Bicycle Counts* dataset from NYC Open Data, focusing on the `counts` column representing the number of cyclists recorded per observation. I only included the first 100k rows, so the analysis was limited.  \n",
    "Using **pandas**, I computed the mean, median, and mode of these counts to summarize the data.  \n",
    "I then repeated these calculations using only Pythonâ€™s standard libraryâ€”reading and processing the raw CSV manually to calculate each value \"the hard way.\"\n",
    "\n",
    "Finally, I created a simple text-based visualization using only built-in Python features, where each ðŸš² emoji represents one cyclist counted per day.  \n",
    "This literal visualization highlights variation in cycling activity across days, showing how code and data can communicate insights without any external plotting libraries.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
